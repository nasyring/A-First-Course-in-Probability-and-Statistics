[["index.html", "A First Course in Probability and Statistics Chapter 1 About", " A First Course in Probability and Statistics Nick Syring 2022-01-28 Chapter 1 About This is a collection of notes intended for students studying probability and statistics at the advanced undergraduate level at Iowa State University. Specifically, these notes are modeled after course notes for STAT 588, 341, and 342. This site is a work in progress. Some relevant textbook references include John E. Freund's Mathematical Statistics with Applications by Miller and Miller, although many books on this material are available. "],["experiments-and-the-role-of-probability.html", "Chapter 2 Experiments and the role of probability 2.1 Experiments 2.2 The role of probability 2.3 Exercises", " Chapter 2 Experiments and the role of probability In this chapter we introduce concepts related to scientific studies, data collection, and random sampling. 2.1 Experiments There are many settings in which data is collected and studied. In this course we will limit our focus to random sampling experiments and random sampling, randomized intervention experiments defined below. Both of these settings start with a research question of interest in the the context of a population or set of individuals. For example, in a political poll the population is the set of eligible voters and the research question is &quot;which of two candidates has majority support?&quot; Experiments like polls necessarily only measure an outcome or response (like voter preference) for a limited number of individuals from the population. The collection of observed individuals is the sample. For many reasons relating to, e.g., cost, time, and access, the size of the sample is small compared to the size of the population. On the other hand, there are rare instances in which a whole population is observed, and we call this a census. In a census, all possible information is gathered, whereas in an experiment only a limited subset of information is obtained. We will be concerned only with experiments and how best to use the limited information gathered in order to answer research questions. When only a sample of a population is available a natural question is &quot;how is the sample determined?&quot; If there is freedom in the choice of sample then &quot;how should the sample be chosen?&quot; Intuitively, if we can only observe some of the individuals belonging to the population then we prefer to observe a representative sample of them. Representative means that the heterogeneities/differences between individuals that exist in the population ought to be more or less accurately reflected in the sample. For example, a poll of eligible voters in Iowa is not representative if it includes only registered Democrats, and we would not expect such a poll to accurately reflect the population preference between two candidates. If we have a representative sample from the population then we assume the results of our experiment are generalizable to the population. In other words, the observations we make are characteristic of the population and we would not expect vastly different observations had we obtained a different, similarly representative sample, or if we had observed the whole population. There are different ways to obtain representative samples. One way is via stratified sampling. For example, suppose we knew or could measure the relative population proportions of all factors relevant to voting preference, say, party affiliation, sex, age, income level, education level, and religious affiliation. Then, we could construct a sample of individuals with values of these demographic variables matching the levels present in the population---a representative sample---by randomly selecting the right number of individuals from each of these subgroupings or strata. However, this takes a lot of information to pull off. Instead, researchers typically attempt to construct a simple random sample of individuals from the population. A simple random sample (SRS) of size \\(n\\) from a finite population means every subset of \\(n\\) individuals is equally likely to be chosen. You can imagine a SRS as pulling numbers out of a hat, so to speak. An SRS of eligible voters could be constructed, for instance, by randomly choosing 100 social security numbers from the list of SSNs of all eligible voters. SRSs are not always trivial to construct---as in the polling example they require a list of population individuals, which may be difficult to obtain. We are at the point where we can define a random sampling experiment as the process by which we select a SRS from a population and record a response from sampled individuals for the purpose of answering a research question. As noted, an important example is a poll. Random sampling experiments are valuable, but limited to addressing questions about one variable at a time. In many cases researchers are interested in how one variable affects the value of another, and these questions can often be answered using interventional experiments. An intervention is an act by a researcher intended to affect the value of the response variable in sampled individuals. When an intervention is applied to samples the samples are often referred to as experimental units. An important example of an interventional experiment is a clinical trial. In a basic clinical trial a random sample of patients is recruited from a population of patients with a certain medical condition. Then, a subset of the patients is given a treatment of interest --- the intervention --- while the other patients are given a different treatment or perhaps no treatment at all (maybe a placebo). The response, probably related to the health of patients post-treatment, is then compared between the intervention and non-intervention groups. This experiment is like a random sampling experiment with an added intervention step. The key question is &quot;how do we determine which experimental units receive the intervention?&quot; Recall that when we obtain a sample of individuals from a population we wish to do so in such a way as to obtain a representative sample. The same idea applies to interventions. The group of samples receiving the intervention should be similar to the group not receiving the intervention; that is, both groups should be representative of the total set of sampled individuals. Therefore, it makes sense to randomize the application of the intervention over sampled individuals. A random sampling, randomized intervention experiment consists of obtaining a SRS from a population, randomizing an intervention over those samples, and recording a response variable relating to a research question. This type of experiment is often explained by conceptualizing two (or more) different populations. Suppose the experiment is a clinical trial and the intervention consists of receiving a treatment versus no treatment. Then, we can interpret the randomization step as defining two SRSs from two (abstract or hypothetical) populations: a sample from the set of treated patients and a sample from the set of untreated patients. The research question typically concerns the relationship between these two populations, such as, &quot;are treated patients, on average, healthier than untreated patients, with respect to the response variable?&quot; We will only consider randomized interventional experiments. Briefly, consider what can go wrong if we do not randomize intervention. Suppose the patients in our clinical trial consist of both old and young people and that old people are generally more in need of treatment. If we give the treatment to only young(old) people, then we will underestimate(overestimate) the effect of the treatment compared to what its effect would be, on average, over the whole population. In effect, we have confounded the treatment/intervention effect with the effect of age on the response. This may seem like a silly example because we could easily avoid assigning the intervention to only young or only old people. But, not all potential confounding variables are obvious/visible. Unknown confounders, also called lurking variables, can only be systematically accounted for via randomizing the intervention. Of course it is possible for randomized groups not to be representative in a particular occurrence, but, systematically, randomized groups will tend to be representative, so it's a good practice. When the intervention is randomized we cautiously assume substantial differences in the observed responses between intervention groups can be attributed to the intervention. In other words, we believe in causation---that the intervention caused the observed differences. When confounding variables are present we never know which variable is responsible for observed differences in responses and we cannot support claims about cause and effect relationships using the data alone. In some studies researchers analyze the relationships between variables without performing randomized interventions; these are usually observational studies. For instance, in the Framingham Heart Study, researchers recorded the health and lifestyle choices of Massachusetts residents over several years. By collecting a large amount of data they were able to establish a strong relationship or association between cigarette smoking and cancer. Their data alone is not enough to imply causation, but their study inspired many follow-up experiments like lab experiments on animals, and the development of biochemical theories about tumor development. The combination of these various works leaves little doubt today that tobacco use dramatically increases the likelihood of developing cancer. 2.2 The role of probability Consider the example of a political poll on the preferences of voters between two candidates---this is a random sampling experiment. The population consists of all eligible voters in the upcoming election. Each voter can be associated, say, with a 1 or a 0, indicating their preference between the two candidates. Let these 0-1 preferences be denoted \\(x_1, \\ldots, x_N\\) where \\(N\\) is the population size. There exists a population level value \\(\\theta\\) equal to \\[\\theta:=N^{-1}\\sum_{j=1}^N x_j\\] denoting the population voter preference; this is the population proportion. Most research questions of interest concern the unknown value of \\(\\theta\\). In the polling experiment we observe a random subset of \\(x_1, \\ldots, x_N\\) values, say, \\(X_1, \\ldots, X_n\\) for \\(n&lt;N\\)---keep in mind these are not the first \\(n\\) x's, but a random subset of \\(n\\) x's. Correspondingly, we can compute the sample voter preference \\[\\hat\\theta_n := n^{-1}\\sum_{i=1}^n X_i.\\] For every possible sample of \\(n\\) voters, there is a corresponding value of \\(\\hat\\theta_n\\). We can think of the polling experiment as randomly choosing one of these \\(\\hat\\theta_n\\) values out of a hat containing all of them. Randomly sampling voters causes randomness in the observed \\(\\hat\\theta_n\\) preference value. The mathematics of probability is used to quantify this randomness, e.g., to be able to compute the chance of \\(\\hat\\theta_n\\) taking on any particular value, given knowledge of the population. That last phrase &quot;given knowledge of the population&quot; is very important, and illustrates the difference between probability and statistics (inference) quite succinctly. Probability characterizes the chance of observing a particular random sample given the population, whereas statistics seeks to explain some characteristic of the unknown/unobserved population given only one sample (subset of population individuals). Probability plays a key role in statistics problems, and the first part of our course is devoted to developing methods for computing probabilities of samples in a variety of useful special cases. Let's illustrate this interplay of probability and statistics by continuing the example of a poll. The population, again, can be represented by \\(N\\) values, each either a \\(0\\) or a \\(1\\) indicating each voter's preferences, and we can label these \\(x_1, \\ldots, x_N\\). A poll is a random sample of \\(n\\) of these values, labeled \\(X_1, \\ldots,X_n\\), without replacement, i.e., once a value \\(x_j\\) is selected and recorded it cannot be selected again (no double voting!). The population preference \\(\\theta\\) is the average of \\(x_j\\)'s for \\(j=1, \\ldots, N\\). If we knew the size of the population \\(N\\) and the population proportion \\(\\theta\\), then we could compute the chance of observing any \\(\\hat\\theta_n := n^{-1}\\sum_{i=1}^n X_i\\) value according to the rules of probability (which we will soon study). For example, if \\(N\\) is much larger than \\(n = 10\\) and \\(\\theta = 1/2\\), then the chance of observing exactly \\(\\hat\\theta = 1/2\\) is about \\(25\\%\\) (it's about equal to \\(252\\theta^5(1-\\theta)^5\\)). Of course, the whole point of the poll is to learn something about the unknown value of \\(\\theta\\), so we cannot actually perform this probability calculation in practice. But, consider connecting the probability calculation to our goal. We would like to distinguish between values of \\(\\theta\\) that are more or less plausible. Suppose we conduct the poll of \\(10\\) individuals and observe \\(\\hat\\theta = 1/2\\). Then, our probability calculation says the probability we observe \\(\\hat\\theta = 1/2\\) is highest if \\(\\theta = 1/2\\). In other words, \\(1/2\\) is the most plausible value of the population proportion given our observations. Intuitively, we expect values near \\(1/2\\) are more plausible than values far from \\(1/2\\). This correspondence between the probability calculation and plausible values of the population parameter is called the maximum likelihood principle which we will study in a later chapter. 2.3 Exercises Google James Lind's Scurvy experiments. What was James' research question? What was the population? Did he obtain a random sample from the population? If not, does that make you suspicious of his findings? Why or why not? Did James use any interventions? If so did he randomize them? Do you think his randomization scheme is reliable? What were his conclusions? Find a recent example of an experiment in the news or a scientific publication. Describe the research question, population, intervention (if there is one), and response. Is it a random sampling experiment, a random sample randomized intervention experiment, or something else, like an observational study? The Challenger space shuttle exploded when it experienced o-ring failures thought to be caused by low launch temperature. The launch temperature was 31 degrees Fahrenheit. The following data can be interpreted as a random sample of counts of o-ring failures from a population of launches. Given this data do you think we can make reliable conclusions about launch safety at 31 degrees launch temperature? What concepts discussed in this section are relevant here? Table 2.1: A table of o rings at risk, o ring failires, and launch temperatures of space shuttle flights. at risk failed launch temp. 6 1 70 6 0 69 6 0 68 6 0 67 6 0 72 6 0 73 6 0 70 6 1 57 6 1 63 6 1 70 6 0 78 6 0 67 6 2 53 6 0 67 6 0 75 6 0 70 6 0 81 6 0 76 6 0 79 6 0 75 6 0 76 6 1 58 "],["probability-and-counting.html", "Chapter 3 Probability and Counting 3.1 Terminology 3.2 Set relations 3.3 Probability Axioms 3.4 Equally likely outcomes 3.5 Some counting rules 3.6 Applications to random sampling 3.7 Exercises", " Chapter 3 Probability and Counting Probability is a tricky subject. We have an intuitive sense about probability, but we use it in different ways, which can sometimes lead to confusion. Probability is used in at least two ways: -to describe the relative frequency of events, e.g., what is the chance of observing 5 heads in the next ten flips of a fair coin; and, -to communicate degrees of belief, e.g., the Packers have a 30% chance of winning the Super Bowl. We will use probability exclusively in the sense of the first interpretation---to characterize the chances of different outcomes in repeatable trials. This sense of probability corresponds to characterizing the possible outcomes of random sampling. Although probability is often used to communicate degrees of belief there are good reasons not to use it for this purpose, but a formal, nuanced discussion of quantification of beliefs is outside our present purview. 3.1 Terminology In our discussion of probability we will think of an experiment as the act of measuring/observing a variable on one or more random samples from a population. The sample space is the set of possible realizations of the experiment. For example, if the experiment is to flip one coin and record whether it is heads or tails, then we can think of this as a random sampling from sample space \\(\\{H, T\\}\\) where the outcome may be either \\(\\{H\\}\\) or \\(\\{T\\}\\). Any subset of the sample space of an experiment is an event; for example, \\(\\{H\\}\\) and \\(\\{H,T\\}\\) are events, and so is \\(\\emptyset\\) which denotes the &quot;empty set&quot;, the set of nothing. 3.2 Set relations Events and sample spaces are sets, and we will make use of relations between sets. -Set union: for sets/events A and B, \\(A\\cup B\\) denotes the set of elements in at least one of \\(A\\) or \\(B\\). For example, \\(\\{1,2,3\\}\\cup\\{3,4,5\\} = \\{1,2,3,4,5\\}\\). -Set intersection: \\(A\\cap B\\) denotes the set of elements in both A and B. For example, \\(\\{1,2,3\\}\\cap\\{3,4,5\\} = \\{3\\}\\). -Set complement: \\(A^c\\) denotes the set of elements in the sample space \\(\\mathcal{S}\\) but not in \\(A\\). For example, if \\(\\mathcal{S} = \\{1,2,3,4,5\\}\\) then \\(A^c = \\{4,5\\}\\). -Set subtraction: \\(A\\backslash B\\) of \\(A-B\\) means \\(A\\cap B^c\\) which is the set of elements in \\(A\\) but not in \\(B\\). For example, \\(\\{1,2,3\\}-\\{3,4,5\\} = \\{1,2\\}\\). 3.2.1 Sample space example Here's an example to illustrate sample spaces. A gas station has six pumps, A, B, C, D, E, F. -What is the sample space of the number of pumps in use? \\(\\mathcal{S} =\\{0,1,2,3,4,5,6\\}\\). -What is the sample space of pumps in use? \\(\\mathcal{S} = \\{\\{A,B,C,D,E,F\\},\\{A,B,C,D,E\\}, \\ldots,\\{F\\}, \\emptyset \\}\\). This is the power set of \\(\\{A,B,C,D,E,F\\}\\), the set of all subsets of those pumps. Fun fact: if the sample space \\(\\mathcal{S}\\) consists of \\(N\\) elements then its power set, written \\(2^{\\mathcal{S}}\\), contains \\(2^N\\) elements. Can you see why? -Suppose you test pump A every day until it fails to function. What is the sample space of this experiment? \\(\\mathcal{S} = \\{F, SF, SSF, \\ldots \\}\\). This is a countably infinite sample space. -You measure the amount of gas pumped by the next customer. \\(\\mathcal{S} = (0, ?)\\), an interval with some upper bound equal to however much gas the station has available. This is an uncountably infinite sample space. 3.2.2 Set relations example This next example illustrates set relations. Consider this system of series and parallel components. Each component either functions/succeeds (S) or fails (F). The experiment simply observes if the system functions/succeeds or fails. -What is the sample space in terms of the three components? \\(\\mathcal{S} = \\{SSS, SSF, SFS, FSS, SFF, FFS, FSF, FFF\\}\\). -Find the even two components succeed. \\(A = \\{SSF, SFS, FSS\\}\\). -Find the even at leas two components succeed. \\(B = \\{SSF, SFS, FSS, SSS\\} = A\\cup \\{SSS\\}\\). -Find the event the system functions. \\(C = \\{SSS, SFS, SSF\\}\\). 3.3 Probability Axioms Andrey Kolmogorov formalized rules/axioms of probability we are mostly, intuitively familiar with. The probability of the sample space is 1, \\(P(\\mathcal{S})=1\\). Probabilities are non-negative. If \\(A\\subset\\mathcal{S}\\) then \\(P(A)\\geq 0\\). Countable additivity. This last one is a bit tricky. Let's start with an intuitive, simpler case. Suppose \\(\\mathcal{S} = \\{1,2,3,4,5\\}\\). The important part is \\(\\mathcal{S}\\) is finite and includes only different things that don't &quot;overlap&quot;. Then, we know that, for example, \\(P(\\{1\\}\\cup \\{2\\}) = P(\\{1\\}) + P(\\{2\\})\\), which says that the probability of the union of disjoint events equals the sum of probabilities of each event. Kolmogorov requires this extends to countably infinite \\(\\mathcal{S}\\), hence the name countable additivity. Specifically, let events \\(A_1\\), \\(A_2\\), \\(\\ldots\\) be a sequence of mutually disjoint events (none overlap, like pizza slices) so that \\(A_i \\cap A_j = \\emptyset\\) for every \\(i\\ne j\\). Then, \\[P\\left(\\bigcup_{i=1}^\\infty A_i\\right) = \\sum_{i=1}^\\infty P(A_i).\\] There are a number of important consequences of the axioms: * Complementarity - \\(P(A^c) = 1-P(A)\\) * Inclusion-exclusion principle \\[P(A\\cup B) = P(\\{A\\cap B\\}\\cup\\{A-B\\}\\cup\\{B-A\\})\\] \\[ = P(A)+P(B)-P(A\\cap B)\\] 3.3.1 Example of using the probability axioms Suppose we are inspecting a product for defects. Three types of defects are possible. Let \\(A_j\\), \\(j=1, 2, 3\\) denote the events that a defect of type \\(j\\) is present. We are given \\(P(A_1) = 12\\%\\), \\(P(A_2) = 7\\%\\), \\(P(A_3) = 5\\%\\), \\(P(A_1\\cup A_2) = 13\\%\\), \\(P(A_1 \\cup A_3) = 14\\%\\), \\(P(A_2 \\cup A_3) = 10\\%\\), and \\(P(A_1\\cap A_2\\cap A_3) = 1\\%\\). 1. Find \\(P(A_1^c)\\). \\[ = 1-P(A_1) = 88\\%\\] 2. Find \\(P(A_1\\cap A_2)\\) \\[ = P(A_1) + P(A_2) - P(A_1\\cup A_2) = 6\\%\\] 3. Find \\(P(A_1 \\cap A_2 \\cap A_3^c)\\) \\[ = P(A_1\\cap A_2) - P(A_1\\cap A_2 \\cap A_3)\\] \\[ = 6\\% - 1\\% = 5\\%\\] 4. Find the probability the system has at most 2 defects. &quot;At most 2&quot; means &quot;Not 3&quot;, so \\[P(\\text{at most 2 defects}) = 1 - 1\\% = 99\\%.\\] 3.4 Equally likely outcomes The axioms of probability lay some ground rules probabilites must follow, but don't say much about how to assign probabilities to events. Next up, we'll see how to determine probabilities of events for finite sample spaces. The principle of equally likely outcomes says that a finite sample space \\(\\mathcal{S}\\) of \\(N\\) disjoint outcomes or elements has equally likely outcomes if the probability of each outcome is \\(1/N\\). It's clear that this probability assignment obeys the axioms. From here we can assign probabilites to events \\(A\\) in \\(\\mathcal{S}\\) by simply counting how many outcomes are contained in \\(A\\); that is, \\[P(A) = \\frac{N(A)}{N}\\] where \\(N(A)\\) denotes the number of outcomes in \\(\\mathcal{S}\\) contained in \\(A\\). Therefore, in the context of equally likely outcomes, counting is very important. Let's take a moment to consider why equally likely outcomes are an important case. We are studying random sampling experiments, and a SRS is defined by the property that every subset of \\(n\\) population individuals is equally likely to be chosen. So, there's a direct correspondence between the probability setup we're considering here and random sampling experiments. 3.5 Some counting rules The product rule considers the probability of a complex event that can be decomposed into several independent events. For example, suppose the event is generating a random &quot;word&quot; that is five (English) letters long. We can decompose this event into randomly sampling (with replacement) from the alphabet of 26 letters 5 times. Each letter we sample is sampled independently (each letter chosen has no influence on the other choices/random draws). The product rule says the number of ways to generate the word (the number of ways the event can happen) is equal to the product of the numbers of ways each letter can be randomly selected (the number of ways each sub-event can happen). For the word example, that's \\(26^5\\). Example: You are remodeling your kitchen and have to buy a new refrigerator, oven, and dishwasher. Four brands make fridges, 3 make ovens, and 5 dishwashers. In how many ways can you select three brands? \\(4\\times 3\\times 5 = 60\\). A combination is a rule for counting the number of subsets of \\(k\\) outcomes/items that can be selected from a larger set of \\(n\\) distinct items. There are \\(\\frac{n!}{k!(n-k)!}\\) also written \\({n \\choose k}\\) such selections. A permutation is similar; it's the number of ways one can select \\(k\\) distinct items from \\(n\\) in a particular order. This is \\(\\frac{n!}{k!}\\). Example: Suppose you win 3 tickets to an NFL playoff game (I wish!). You will choose 2 out of 4 of your closest friends to go with you. How many different choices can you make? It's \\({4\\choose 2} = \\frac{4*3*2*1}{(2*1)(2*1) } = 6\\) choices. Example: In ranked-choice voting you rank your choices by order of preference rather than by voting for only one option. If there are ten options, then how many top four rankings are possible? In this case, it's \\(\\frac{10!}{4!}\\) because we consider different orderings to be distinct, of course. Next we consider a rule for counting objects of different types. Suppose we have \\(n\\) objects: \\(n_1\\) of type 1, \\(n_2\\) of type 2, ... and \\(n_k\\) of type \\(k\\) such that \\(n=\\sum_{j=1}^k n_j\\). Items of the same type are indistinguishable. The number of distinct/distinguishable arrangements of the \\(n\\) objects is \\(\\frac{n!}{n_1!\\times n_2! \\times \\cdots \\times n_k! }\\). For example, if we file a 6-sided die 20 times and obtain 6 ones 3 twos 4 threes and 7 fours the number of distinct orderings of those outcomes is \\(\\frac{20!}{6!3!4!7!}\\). 3.6 Applications to random sampling Consider flipping a fair coin \\(20\\) times. This can be thought of as an event decomposed into 20 independent sub-events---the different flips. The product rule says there are \\(N = 2^{20}\\) possible outcomes. Next, consider how many outcomes have \\(5\\) heads. This is the number of distinct arrangements of 5 heads and 15 tails, which equals \\(\\frac{20!}{5!15!}\\). Putting these together, the probability of exactly 5 heads in 20 flips is \\[\\frac{20!}{5!15!}(\\frac{1}{2})^{20}.\\] We can generalize this to \\(n\\) flips and \\(x\\) heads, easily enough... The probability of \\(x\\) heads in \\(n\\) flips is \\[P(x) = \\frac{n!}{x!(n-x)!}(\\frac{1}{2})^n.\\] This coin-flipping experiment is an example of random sampling with replacement. Each flip is a random draw from a population of \\(2\\) coins in which \\(1\\) is heads and \\(1\\) is tails. Each coin is equally likely to be chosen. And, once selected and recorded, the coin is then returned. Alternatively, we could imagine the experiment as random sampling without replacement from an infinite population of coins for which &quot;half&quot; are heads. For a more concrete example of sampling without replacement, consider the following. Suppose the statistics department has 15 microsoft and 10 mac laptops available for lending, and suppose 6 are chosen as a SRS. What is the probability 3 of the chosen laptops are microsoft and the other 3 mac? There are \\(N = {25 \\choose 6}\\) ways to select 6 at random. There are \\({15 \\choose 3}\\) ways to select 3 microsoft and \\({10 \\choose 3}\\) ways to select \\(3\\) mac laptops. Therefore, the probability is the ratio \\[\\frac{{15 \\choose 3}\\times {10 \\choose 3}}{{25 \\choose 6}}.\\] 3.7 Exercises An Amazon warehouse employs twenty workers on early shift, 15 on the late shift, and 10 on the overnight shift. Six workers are randomly selected (without replacement) fora safety interview. How many selections result in 6 workers from the early shift? What is the probability of this event? What is the probability all 6 workers selected come from the same shift? What is the probability at least two different shifts are represented among the six workers selected? What is the probability at least one shift is not represented? Answers: a. \\({20 \\choose 6}\\), and \\(\\frac{{20 \\choose 6}}{{45 \\choose 6}}\\) b. \\(\\frac{{20 \\choose 6}+{15 \\choose 6}+{10 \\choose 6}}{{45 \\choose 6}}\\) c. \\(1-\\)the probability in b. d. \\(\\frac{{35 \\choose 6}+{30 \\choose 6}+{25 \\choose 6}}{{45 \\choose 6}}\\) Suppose a chain molecule consists of 3 sub-molecules of type A, 3 of B, 3 of C, and 3 of D, e.g., ABCDABCDABCD. How many such distinguishable chain molecules are there of any order? If we randomly selected a chain molecule, what tis the probability that all three sub-molecules of each type are &quot;lined up&quot;, e.g., BBBAAADDDCCC? Answers: a. \\(\\frac{12!}{(3!)^4}\\) b. \\(\\frac{4!(3!)^4}{12!}\\) 3. A quality control inspector must examine a part from each of 4 different bins. The bins contain 5, 2, 6, and 3 parts, respectively. In how many different ways can the inspector choose the 4 parts? In order to meet the requirements of a new housing code, each apartment complex in a certain neighborhood must have the smoke detectors from 3 apartments checked by the fire chief each year. In how many ways can the fire chief choose 3 apartments from a complex containing 20 apartments? A ten person City Council must elect a Chair, a Vice Chair, and a Secretary. In how many ways can the positions be filled? The driving route you use to travel to and from work and your home includes two stop lights. Let \\(A\\) and \\(B\\) denote the events you stop and the first light and the second light, respectively. Suppose \\(P(A) = 0.4\\), \\(P(B) = 0.5\\), and \\(P(A\\cup B) = 0.6\\). Find: \\(P(A\\cap B)\\), the probability you stop at both lights \\(P(A \\cap B^c)\\), the probability you stop at only the first light \\(P(B \\cap A^c)\\), the probability you stop at only the second light "],["conditional-probabilities-of-events.html", "Chapter 4 Conditional probabilities of events 4.1 Bayes' rule 4.2 Independence 4.3 Exercises", " Chapter 4 Conditional probabilities of events Let \\(A\\) and \\(B\\) denote events in a sample space \\(\\mathcal{S}\\). Suppose we know \\(B\\) occurs. Then, \\(P(A|B)\\) denotes the conditional probability of \\(A\\) given \\(B\\). This has the mathematical formula \\[P(A|B) = \\frac{P(A\\cap B)}{P(B)}.\\] In this formula, \\(B\\) acts as the new or conditional sample space. It is always true \\[P(A|\\mathcal{S}) = \\frac{P(A\\cap \\mathcal{S})}{P(\\mathcal{S})} = \\frac{P(A)}{1} = P(A).\\] Knowing \\(B\\) happens is equivalent to changing the sample space to \\(B\\). 4.0.1 Example Four individuals are donating blood to a blood bank. Their blood types are unknown. If exactly 1 of them is O+, then what is the probability at least 3 are typed in order to obtain an O+ sample? Answer: \\[P(\\text{at least three must be typed}) = P(3) + P(4).\\] \\[P(3) = P(\\text{first is not O+ and second is not O+ and third is O+}).\\] \\[ = P(\\text{first is not O+})P(\\text{second is not O+ given first is not O+})P(\\text{third is O+ given first two are not})\\] \\[ = \\frac{3}{4}\\times \\frac{2}{3}\\times \\frac{1}{2}.\\] We used a (perhaps intuitive) extension of the conditional probability formula above: \\[P(A\\cap B\\cap C) = P(C|A\\cap B)P(A \\cap B) = P(C|A\\cap B)P(B|A)P(A)\\] Here, \\(A\\), \\(B\\), and \\(C\\) represent the first, second, and third blood typings. 4.0.2 Example Let \\(A\\) be the event a randomly selected adult male US citizen is at least 6'3&quot; tall. Let \\(B\\) be the event a randomly selected adult male US citizen is an NBA player. Which is larger, \\(P(A|B)\\) or \\(P(B|A)\\)? Answer: Be careful; these can be tricky. \\(P(A)\\) is much larger than \\(P(B)\\) because there are many more male US citizens 6'3&quot; or taller than there are NBA players (some also may not be US citizens). And, \\(P(A|B)\\) is much larger than \\(P(B|A)\\). The reason is that if we condition on US citizen NBA players, almost all of them are 6'3&quot; or taller. However, if we condition on US males 6'3&quot; or taller, NBA players are still a tiny proportion. 4.0.3 Example Bertrand's Boxes --- There are three boxes: the first has two gold coins, the second one gold and one silver, and the third two silver coins. You choose a box at random and a coin from the box at random. Given you choose a gold coin, what is the probability the other coin in that box is gold? Answer: Let \\(G_1\\) be the event the first coin chosen is gold and let \\(G_2\\) be the event the second coin chosen (same box) is gold. Let \\(B_1\\), \\(B_2\\), and \\(B_3\\) denote events corresponding to choosing boxes 1, 2, and 3. Then, we seek \\[P(G_2|G_1) = \\frac{P(G_1\\cap G_2)}{P(G_1)}.\\] \\[P(G_1) = P(G_1 \\cap B_1) + P(G_1 \\cap B_2) + P(G_1 \\cap B_3)\\] \\[ = P(G_1 | B_1)P(B_1) + P(G_1 | B_2)P(B_2)+ P(G_1 | B_3)P(B_3)\\] \\[ = 1 \\times 1/3 + 1/2 \\times 1/3 + 0 \\times 1 /3 = 1/2.\\] \\[P(G_1\\cap G_2) = P(\\text{chose box 1}) = P(B_1) = 1/3.\\] Therefore, \\[P(G_2|G_1) = \\frac{1/3}{1/2} = 2/3.\\] Note: This question, which is often called &quot;Bertrand's Paradox&quot;, has caused an awful lot of consternation and controversy. When pondering this question for the first time, many people will (incorrectly) suppose the answer is 1/2. Their logic is that since the first coin was gold, then either the first or second box was chosen. Then, those boxes being equally likely, there is a 1/2 chance box 1 was chosen, and, therefore, the chance of the second coin being gold is 1/2. The first part of that logic is well-founded: the third box could not have been chosen. However, the fact the first coin is gold also implies we were more likely to have chosen the first box than the second---those two boxes are not equally likely in light of the information that the first coin was gold. Indeed, we could have simply asked &quot;what is the chance we selected box 1 given the coin is gold?&quot;---that question is equivalent to asking the chance the second coin is gold. When rephrased this way, most people find it easier to see that the chosen box is more likely box 1 than box 2 (twice as likely, in particular), given the first coin selected is gold. 4.1 Bayes' rule In the example of Bertrand's Paradox you might have noticed we used a slick trick to compute \\(P(G_1)\\)---we partitioned the event \\(G_1\\) into three pieces \\(G_1 = \\{G_1 \\cap B_1\\}\\cup \\{G_1 \\cap B_2\\}\\cup \\{G_1 \\cap B_3\\}\\) where \\(\\{B_1 \\cup B_2 \\cup B_3\\} = \\mathcal{S}\\). Partitioning is the act of subdividing a set into non-overlapping, or disjoint, subsets. This way, we can represent a set as a union of subsets, much like a pizza is equivalent to the collection of pizza slices. This partitioning trick is useful enough to be given a name, the Law of Total Probability: If \\(A_1, \\ldots, A_n\\) is a collection of disjoint subsets of \\(\\mathcal{S}\\) such that \\(\\mathcal{S} = \\bigcup_{i=1}^n A_i\\) (i.e., the collection is exhaustive), then for any \\(B\\subset \\mathcal{S}\\) \\[P(B) = \\sum_{i=1}^n P(B \\cap A_i) = \\sum_{i=1}^n P(B | A_i)P(A_i).\\] Recall the axiom of countable additivity; it's basically equivalent to the law of total probability. Often times information is known about certain conditional probabilities---\\(P(A|B)\\)---but we want to know the reverse conditional probability \\(P(B|A)\\). Bayes' rule provides a formula for computing these reverse conditional probabilities. There's not really anything special about the formula; after all, it follows immediately from the definition of conditional probability: \\[P(A|B) = \\frac{P(A\\cap B)}{P(B)} = \\frac{P(A\\cap B)P(A)}{P(B)P(A)} = \\frac{P(B|A)P(A)}{P(B)}.\\] Or, in other words, \\[P(A|B)P(B) = P(B|A)P(A).\\] 4.1.1 Example Perhaps the most famous example of Bayes' rule illustrates the difficulty in detecting rare events. Suppose 1 in 1000 adults suffers from a (fairly rare) disease. A diagnostic test is 99% effective in detecting disease among individuals who have the disease. Among people who do not have the disease, the diagnostic test is nevertheless positive 2% of the time. Given the test is positive, what is the chance the individual tested has the disease? Answer: Let \\(A\\) be the event the test is positive and let \\(B\\) be the event the individual tested has the disease. Then, we are given \\(P(B) = 0.001\\). By the law of total probablity \\[P(A) = P(A\\cap B)+P(A\\cap B^c) = P(A| B)P(B)+P(A| B^c)P(B^c) = 0.99\\times 0.001 + 0.02\\times 0.999 = 0.02097\\] We want to find \\(P(B|A)\\), which is equal to \\(P(A|B)P(B)/P(A)\\) by Bayes' rule. So, \\[P(B|A) = P(A|B)P(B)/P(A) = 0.99\\times 0.001 / 0.02097 = 0.0472103.\\] This is a terrible diagnostic test. Less than 5% of people who test positive actually have the disease. 4.2 Independence Two events \\(A\\) and \\(B\\) are independent if \\[P(A|B) = P(A).\\] In other words, knowing \\(B\\) happens does not affect the chance of \\(A\\) happening. By using the definition of conditional probablity, it follows that \\[P(A|B) = \\frac{P(A\\cap B)}{P(B)} \\stackrel{ind.}{=}P(A)\\Rightarrow P(A\\cap B) = P(A)P(B).\\] Further, if \\(P(A\\cap B) = P(A)P(B)\\) then, again using the definition of conditional probability \\[P(A|B) = \\frac{P(A\\cap B)}{P(B)} = \\frac{P(A)P(B)}{P(B)} = P(A).\\] Therefore, independence of two events \\(A\\) and \\(B\\) is equivalent to the product rule being satisfied: \\[A\\text{ and }B\\text{ independent } \\iff P(A\\cap B) = P(A)P(B).\\] Independence extends to collections of events. For example, intuitively we know two rolls of a die are independent. But, ther'e nothing special about rolling a die twice; three or more rolls are also, intuitively, mutually independent. To be precise, we say a collection of events \\(A_1, A_2, \\ldots\\) is pairwise independent if for every pair \\(A_i, A_j\\), \\(i\\ne j\\) the product rule holds: \\(P(A_i \\cap A_j) = P(A_i)P(A_j)\\). Furthermore, a collection of events \\(A_1, A_2, \\ldots\\) with index set equal to the natural numbers \\(\\mathbb{N}\\) (or a finite subset) is mutually independent if every subset \\(\\{A_i, i\\in I, I\\subset \\mathbb{N}\\}\\) of events obeys the product rule: \\(P( \\bigcap_{i \\in I} A_i) = \\prod_{i \\in I} P(A_i)\\). Example: Consider rolling a fair die \\(n\\) times and recording the outcomes. Let \\(A_i,\\) \\(i = 1,\\ldots,n\\) be the event the \\(i^{th}\\) roll is odd. The set \\(\\{A_i, i=1, \\ldots, n\\}\\) is mutually independent. Example: Let \\(A\\) and \\(B\\) denote the events that the first and second tosses of a fair coin are heads. These are independent events. Define \\(C = \\{\\{A\\cup B\\}-\\{A\\cap B\\}\\}\\). Then, \\(A\\) and \\(C\\) are pairwise independent. To see this, compute: \\[P(A \\cap C) = P(A \\cap \\{\\{A \\cap B^c\\} \\cup \\{A^c \\cap B\\}\\}) = P(A \\cap \\{A \\cap B^c\\}) + P(A \\cap \\{A^c \\cap B\\}) = P(A \\cap B^c) = P(A)(1-P(B)) = 0.25\\] \\[P(C) = P(A \\cap B^c) + P(A^c \\cap B) = P(A) - P(A)P(B) + P(B)- P(A)P(B) = 0.5\\] \\[P(A)P(C) = 0.25\\] But, A, B, and C are not mutually independent because \\[\\{A\\cap B\\cap C\\} = \\{\\{A\\cap B\\}\\cap \\{A \\cap B^c\\}\\} \\cup \\{\\{A\\cap B\\}\\cap \\{A^c \\cap B\\}\\} = \\emptyset\\] So, \\(P(A\\cap B\\cap C) = 0\\), while \\(P(A)P(B)P(C) = 0.125\\). 4.3 Exercises Seventy percent of small, private-use aircraft that disappear in flight are eventually discovered. Of the aircraft that are discovered, \\(60\\%\\) have a black box, whereas \\(90\\%\\) of the missing aircraft that are not discovered do not have a black box. Suppose an aircraft has disappeared. If it has a black box what is the probability it will not be discovered? If it does not have a black box what is the probability it will be discovered? Consider rolling two different fair, six-sided dice, one red and one green. Let \\(A\\) be the event the red die is 3, \\(B\\) be the event the green die is \\(4\\), and \\(C\\) be the event the sum of the two die is \\(7\\). Are \\(A\\) and \\(B\\) independent? Are \\(B\\) and \\(C\\) independent? Are \\(A\\) and \\(C\\) independent? Are the three events mutually independent? "],["random-variables.html", "Chapter 5 Random Variables 5.1 Random variables 5.2 Probability Mass Functions", " Chapter 5 Random Variables 5.1 Random variables So far, we've discussed probabilities for events that are subsets of simple, finite sample spaces. However, often we are interested in continuous measurements/responses on sampled individuals, so we need to expand our probability calculus to accomodate this more general situation. Let \\(X\\) denote a numerical response or function of a random sample from \\(\\mathcal{S}\\). For a basic example, consider the experiment of flipping a fair coin and recording the outcome. Then \\(\\mathcal{S} = \\{H, T\\}\\), but we might map heads and tails to the numbers 1 and 0. Therefore, let \\(X:\\{H,T\\}\\mapsto \\{1,0\\}\\) with \\(X(H) = 1\\) and \\(X(T) = 0\\) denote this mapping/function. \\(X\\) is a random variable---or mapping---defined on the space \\(\\{0,1\\}\\), taking the sample space as domain and some space of numbers as its codomain. In many cases we will consider continuous random variables, mappings with codomain equal to the real numbers, denoted \\(\\mathbb{R}\\), or some (uncountable) subset thereof, like the inteval \\((0,1)\\). In contrast, when the codomain of a random variable is a countable set of numbers, e.g., the positive integers, then we call it a discrete random variable. 5.1.1 Examples Consider an experiment in ehich a mechanical dialer machine randomly dials phone numbers with a given area code. This is a common practice in polling. Let \\(Y\\) denote the random variable with value \\(1\\) if the dialed number is on a &quot;do not call list&quot; and zero otherwise. A random variable (r.v.) that can only take values 0 or 1 is called a Bernoulli random variable. (Oddly enough, a random variable that can only take values 1 or -1 is called a Rademacher r.v. !) A quality control test is used to test the strength of concrete beams. The test exposes beams to both shear forces and flexure forces until each beam fails. An experiment randomly selects three failed beams and observes whether each failed due to shear (S) or flexure (F) forces. The sample space is: \\[\\mathcal{S} = \\{SSS, SSF, SFS, FSS, SFF, FFS, FSF, FFF\\}.\\] Let \\(X\\) denote the random variable that counts the number of beams that fail due to flexure force. It has codomain \\(\\{0,1,2,3\\}\\) and is defined by the mapping \\[X(SSS) = 3, \\, X(SSF) = 1, \\, \\ldots, X(FFF) = 0.\\] In the context of 2. above suppose an experiment randomly selects a failed beam and measures the force \\(X\\) applied at point of failure in Newtons. Consider the sample space of all hypothetical concrete beams that could be tested. Then, \\(X\\) is a continuous r.v. with codomain \\((0,U)\\) for some upper bound \\(U\\). Depending on what is known about the beams, we may take \\(U\\) to be a specified upper bound, or we may set \\(U = \\infty\\). More on this later... 5.2 Probability Mass Functions A probability mass function or PMF is a function that assigns probabilities to values of discrete r.v.'s. We'll write \\(p(x)\\) for the PMF of a r.v. \\(X\\) where \\(x\\) indicates any value in the codomain of \\(X\\). When \\(X\\) is a finite r.v. with only a small number of possible values its PMF can be conveniently expressed as a table of values. For example, in the concrete beam example in 2. above we have \\[x: 0 \\quad 1\\quad 2\\quad 3\\] \\[p(x):1/8\\quad 3/8\\quad 3/8 \\quad 1/8\\] We can also represent PMFs using a function. For example, the probability \\(x\\) heads are observed in 20 flips of a fair coins is given by \\(p(x) = {20 \\choose x}(\\frac{1}{2})^{20}\\). Example: Suppose a contractor needs between 1 and 5 (inclusive) permits for a construction job. If \\(p(x)\\propto x^2\\) then find \\(p(x)\\). The key is that \\(\\sum_{x=1}^5 p(x) = 1\\). Therefore, we need to find the proportionality constant \\(C\\) satisfying \\(C(1^2+2^2+3^2+4^2+5^2) = 1\\). Verify \\(p(x) = x^2 / 55\\). "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
