<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Joint Distributions | A First Course in Probability and Statistics</title>
  <meta name="description" content="These are a collection of notes related to STAT 588 and 341/342 at Iowa State University. This is a work in progress." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Joint Distributions | A First Course in Probability and Statistics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="These are a collection of notes related to STAT 588 and 341/342 at Iowa State University. This is a work in progress." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Joint Distributions | A First Course in Probability and Statistics" />
  
  <meta name="twitter:description" content="These are a collection of notes related to STAT 588 and 341/342 at Iowa State University. This is a work in progress." />
  

<meta name="author" content="Nick Syring" />


<meta name="date" content="2022-02-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="expectation-of-random-variables.html"/>
<link rel="next" href="special-discrete-distributions.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A First Course in Probability and Statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> About</a></li>
<li class="chapter" data-level="2" data-path="experiments-and-the-role-of-probability.html"><a href="experiments-and-the-role-of-probability.html"><i class="fa fa-check"></i><b>2</b> Experiments and the role of probability</a>
<ul>
<li class="chapter" data-level="2.1" data-path="experiments-and-the-role-of-probability.html"><a href="experiments-and-the-role-of-probability.html#experiments"><i class="fa fa-check"></i><b>2.1</b> Experiments</a></li>
<li class="chapter" data-level="2.2" data-path="experiments-and-the-role-of-probability.html"><a href="experiments-and-the-role-of-probability.html#the-role-of-probability"><i class="fa fa-check"></i><b>2.2</b> The role of probability</a></li>
<li class="chapter" data-level="2.3" data-path="experiments-and-the-role-of-probability.html"><a href="experiments-and-the-role-of-probability.html#exercises"><i class="fa fa-check"></i><b>2.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="probability-and-counting.html"><a href="probability-and-counting.html"><i class="fa fa-check"></i><b>3</b> Probability and Counting</a>
<ul>
<li class="chapter" data-level="3.1" data-path="probability-and-counting.html"><a href="probability-and-counting.html#terminology"><i class="fa fa-check"></i><b>3.1</b> Terminology</a></li>
<li class="chapter" data-level="3.2" data-path="probability-and-counting.html"><a href="probability-and-counting.html#set-relations"><i class="fa fa-check"></i><b>3.2</b> Set relations</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="probability-and-counting.html"><a href="probability-and-counting.html#sample-space-example"><i class="fa fa-check"></i><b>3.2.1</b> Sample space example</a></li>
<li class="chapter" data-level="3.2.2" data-path="probability-and-counting.html"><a href="probability-and-counting.html#set-relations-example"><i class="fa fa-check"></i><b>3.2.2</b> Set relations example</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="probability-and-counting.html"><a href="probability-and-counting.html#probability-axioms"><i class="fa fa-check"></i><b>3.3</b> Probability Axioms</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="probability-and-counting.html"><a href="probability-and-counting.html#example-of-using-the-probability-axioms"><i class="fa fa-check"></i><b>3.3.1</b> Example of using the probability axioms</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="probability-and-counting.html"><a href="probability-and-counting.html#equally-likely-outcomes"><i class="fa fa-check"></i><b>3.4</b> Equally likely outcomes</a></li>
<li class="chapter" data-level="3.5" data-path="probability-and-counting.html"><a href="probability-and-counting.html#some-counting-rules"><i class="fa fa-check"></i><b>3.5</b> Some counting rules</a></li>
<li class="chapter" data-level="3.6" data-path="probability-and-counting.html"><a href="probability-and-counting.html#applications-to-random-sampling"><i class="fa fa-check"></i><b>3.6</b> Applications to random sampling</a></li>
<li class="chapter" data-level="3.7" data-path="probability-and-counting.html"><a href="probability-and-counting.html#exercises-1"><i class="fa fa-check"></i><b>3.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="conditional-probabilities-of-events.html"><a href="conditional-probabilities-of-events.html"><i class="fa fa-check"></i><b>4</b> Conditional probabilities of events</a>
<ul>
<li class="chapter" data-level="4.0.1" data-path="conditional-probabilities-of-events.html"><a href="conditional-probabilities-of-events.html#example"><i class="fa fa-check"></i><b>4.0.1</b> Example</a></li>
<li class="chapter" data-level="4.0.2" data-path="conditional-probabilities-of-events.html"><a href="conditional-probabilities-of-events.html#example-1"><i class="fa fa-check"></i><b>4.0.2</b> Example</a></li>
<li class="chapter" data-level="4.0.3" data-path="conditional-probabilities-of-events.html"><a href="conditional-probabilities-of-events.html#example-2"><i class="fa fa-check"></i><b>4.0.3</b> Example</a></li>
<li class="chapter" data-level="4.1" data-path="conditional-probabilities-of-events.html"><a href="conditional-probabilities-of-events.html#bayes-rule"><i class="fa fa-check"></i><b>4.1</b> Bayes’ rule</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="conditional-probabilities-of-events.html"><a href="conditional-probabilities-of-events.html#example-3"><i class="fa fa-check"></i><b>4.1.1</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="conditional-probabilities-of-events.html"><a href="conditional-probabilities-of-events.html#independence"><i class="fa fa-check"></i><b>4.2</b> Independence</a></li>
<li class="chapter" data-level="4.3" data-path="conditional-probabilities-of-events.html"><a href="conditional-probabilities-of-events.html#exercises-2"><i class="fa fa-check"></i><b>4.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="random-variables.html"><a href="random-variables.html"><i class="fa fa-check"></i><b>5</b> Random Variables</a>
<ul>
<li class="chapter" data-level="5.1" data-path="random-variables.html"><a href="random-variables.html#random-variables-1"><i class="fa fa-check"></i><b>5.1</b> Random variables</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="random-variables.html"><a href="random-variables.html#examples-of-discrete-r.v.s"><i class="fa fa-check"></i><b>5.1.1</b> Examples of Discrete r.v.’s</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="random-variables.html"><a href="random-variables.html#probability-mass-functions"><i class="fa fa-check"></i><b>5.2</b> Probability Mass Functions</a></li>
<li class="chapter" data-level="5.3" data-path="random-variables.html"><a href="random-variables.html#cumulative-mass-functions"><i class="fa fa-check"></i><b>5.3</b> Cumulative Mass Functions</a></li>
<li class="chapter" data-level="5.4" data-path="random-variables.html"><a href="random-variables.html#examples-of-continuous-r.v.s"><i class="fa fa-check"></i><b>5.4</b> Examples of Continuous r.v.’s</a></li>
<li class="chapter" data-level="5.5" data-path="random-variables.html"><a href="random-variables.html#probability-assignments-for-continuous-r.v.s"><i class="fa fa-check"></i><b>5.5</b> Probability assignments for continuous r.v.’s</a></li>
<li class="chapter" data-level="5.6" data-path="random-variables.html"><a href="random-variables.html#transformations-of-random-variables"><i class="fa fa-check"></i><b>5.6</b> Transformations of random variables</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="expectation-of-random-variables.html"><a href="expectation-of-random-variables.html"><i class="fa fa-check"></i><b>6</b> Expectation of Random Variables</a>
<ul>
<li class="chapter" data-level="6.1" data-path="expectation-of-random-variables.html"><a href="expectation-of-random-variables.html#mean-of-a-random-variable"><i class="fa fa-check"></i><b>6.1</b> Mean of a Random Variable</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="expectation-of-random-variables.html"><a href="expectation-of-random-variables.html#examples"><i class="fa fa-check"></i><b>6.1.1</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="expectation-of-random-variables.html"><a href="expectation-of-random-variables.html#variance-of-a-random-variable"><i class="fa fa-check"></i><b>6.2</b> Variance of a random variable</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="expectation-of-random-variables.html"><a href="expectation-of-random-variables.html#examples-1"><i class="fa fa-check"></i><b>6.2.1</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="expectation-of-random-variables.html"><a href="expectation-of-random-variables.html#special-uses-of-mean-and-variance"><i class="fa fa-check"></i><b>6.3</b> Special uses of mean and variance</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="expectation-of-random-variables.html"><a href="expectation-of-random-variables.html#examples-2"><i class="fa fa-check"></i><b>6.3.1</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="expectation-of-random-variables.html"><a href="expectation-of-random-variables.html#expectations-of-functions-of-random-variables"><i class="fa fa-check"></i><b>6.4</b> Expectations of functions of random variables</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="joint-distributions.html"><a href="joint-distributions.html"><i class="fa fa-check"></i><b>7</b> Joint Distributions</a>
<ul>
<li class="chapter" data-level="7.1" data-path="joint-distributions.html"><a href="joint-distributions.html#jointly-distributed-discrete-random-variables"><i class="fa fa-check"></i><b>7.1</b> Jointly distributed discrete random variables</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="joint-distributions.html"><a href="joint-distributions.html#marginal-pmfs"><i class="fa fa-check"></i><b>7.1.1</b> Marginal PMFs</a></li>
<li class="chapter" data-level="7.1.2" data-path="joint-distributions.html"><a href="joint-distributions.html#conditional-pmfs"><i class="fa fa-check"></i><b>7.1.2</b> Conditional PMFs</a></li>
<li class="chapter" data-level="7.1.3" data-path="joint-distributions.html"><a href="joint-distributions.html#independence-of-discrete-random-variables"><i class="fa fa-check"></i><b>7.1.3</b> Independence of discrete random variables</a></li>
<li class="chapter" data-level="7.1.4" data-path="joint-distributions.html"><a href="joint-distributions.html#expectations-involving-multiple-discrete-random-variables"><i class="fa fa-check"></i><b>7.1.4</b> Expectations involving multiple discrete random variables</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="joint-distributions.html"><a href="joint-distributions.html#jointly-distributed-continuous-random-variables"><i class="fa fa-check"></i><b>7.2</b> Jointly distributed continuous random variables</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="joint-distributions.html"><a href="joint-distributions.html#marginal-densities"><i class="fa fa-check"></i><b>7.2.1</b> Marginal densities</a></li>
<li class="chapter" data-level="7.2.2" data-path="joint-distributions.html"><a href="joint-distributions.html#conditional-densities"><i class="fa fa-check"></i><b>7.2.2</b> Conditional densities</a></li>
<li class="chapter" data-level="7.2.3" data-path="joint-distributions.html"><a href="joint-distributions.html#independence-of-jointly-distributed-continuous-r.v.s"><i class="fa fa-check"></i><b>7.2.3</b> Independence of jointly-distributed continuous r.v.’s</a></li>
<li class="chapter" data-level="7.2.4" data-path="joint-distributions.html"><a href="joint-distributions.html#expectations-involving-more-than-one-continuous-r.v."><i class="fa fa-check"></i><b>7.2.4</b> Expectations involving more than one continuous r.v.</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="special-discrete-distributions.html"><a href="special-discrete-distributions.html"><i class="fa fa-check"></i><b>8</b> Special Discrete Distributions</a>
<ul>
<li class="chapter" data-level="8.1" data-path="special-discrete-distributions.html"><a href="special-discrete-distributions.html#bernoulli-distribution"><i class="fa fa-check"></i><b>8.1</b> Bernoulli Distribution</a></li>
<li class="chapter" data-level="8.2" data-path="special-discrete-distributions.html"><a href="special-discrete-distributions.html#categorical-distribution"><i class="fa fa-check"></i><b>8.2</b> Categorical Distribution</a></li>
<li class="chapter" data-level="8.3" data-path="special-discrete-distributions.html"><a href="special-discrete-distributions.html#binomial-distribution"><i class="fa fa-check"></i><b>8.3</b> Binomial distribution</a></li>
<li class="chapter" data-level="8.4" data-path="special-discrete-distributions.html"><a href="special-discrete-distributions.html#multinomial-distribution"><i class="fa fa-check"></i><b>8.4</b> Multinomial Distribution</a></li>
<li class="chapter" data-level="8.5" data-path="special-discrete-distributions.html"><a href="special-discrete-distributions.html#hypergeometric-distribution"><i class="fa fa-check"></i><b>8.5</b> Hypergeometric distribution</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A First Course in Probability and Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="joint-distributions" class="section level1" number="7">
<h1><span class="header-section-number">Chapter 7</span> Joint Distributions</h1>
<div id="jointly-distributed-discrete-random-variables" class="section level2" number="7.1">
<h2><span class="header-section-number">7.1</span> Jointly distributed discrete random variables</h2>
Suppose <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> denote two discrete random variables. We want to compute the probability each takes a specific value <em>at the same time</em> or <em>simultaneously</em>. Denote this <em>joint</em> probability by <span class="math inline">\(P(X_1 = x_1, \, X_2 = x_2)\)</span>. As we’ve previously seen, probabilities of discrete random variables behave just like probabilities of events, so this joint probability is analogous to <span class="math inline">\(P(A\cap B)\)</span>. Joint probabilities are assigned by a joint PMF. In cases where the two (or more) random variables take on only finitely many values, their joint PMF can be displayed in a table.
<br><br>
Example: An insurance company randomly selects one of their customers and records <span class="math inline">\(X_1\)</span>, the value of their deductible on their auto policy and <span class="math inline">\(X_2\)</span>, the value of their deductible on their home policy. Suppose the following PMF characterizes the joint probabilities of <span class="math inline">\((X_1, X_2)\)</span>:<br>
<center>
<p><img src="joint_pmf.PNG" style="width:20.0%" /></p>
</center>
<p>Then, for example, the probability the customer’s auto deductible is 250 and home deductible is 100 is <span class="math inline">\(P(X_1 = 250, \, X_2 = 100) = p(250,100) = 0.15\)</span>.</p>
<p>In other cases joint PMFs may be derived using counting rules as the following example illustrates. <br><br></p>
<p>Example: Previously we used counting rules to find the PMF of the number of heads (or tails) in <span class="math inline">\(n\)</span> flips of a coin with heads probability of <span class="math inline">\(p\)</span> (usually <span class="math inline">\(p=1/2\)</span>). That PMF is <span class="math inline">\(p(x) = {n \choose x}p^x (1-p)^{n-x}\)</span>. Now, suppose we roll a fair, six-sided die <span class="math inline">\(n\)</span> times, and let <span class="math inline">\(X_1\)</span> denote the number of 1’s and <span class="math inline">\(X_2\)</span> denote the number of 2’s. What is the PMF of the probability we get <span class="math inline">\(x_1\)</span> 1’s and <span class="math inline">\(x_2\)</span> 2’s?<br><br></p>
<p>Solution: The chance of rolling a 1 is 1/6, and so is the chance of rolling a 2. So, provided <span class="math inline">\(x_1 + x_2 \leq n\)</span> we have <span class="math inline">\((\tfrac16)^{x_1}(\tfrac16)^{x_2}(\tfrac46)^{n-x_1-x_2}\)</span> probability of any string of rolls of <span class="math inline">\(x_1\)</span> 1’s, <span class="math inline">\(x_2\)</span> 2’s, and remaining rolls. Then, we have to count the number of ways of arranging said strings of rolls, and for this we use the formula for counting arrangements of distinguishable and indistinguishable objects — that is, <span class="math inline">\(\frac{n!}{x_1! x_2! (n-x_1 -x_2)!}\)</span>. Therefore, the PMF is given by
<span class="math display">\[p(x_1, x_2) = \frac{n!}{x_1! x_2! (n-x_1 -x_2)!}(\tfrac16)^{x_1}(\tfrac16)^{x_2}(\tfrac46)^{n-x_1-x_2}\]</span></p>
<div id="marginal-pmfs" class="section level3" number="7.1.1">
<h3><span class="header-section-number">7.1.1</span> Marginal PMFs</h3>
<p>Given a joint PMF <span class="math inline">\(p(x_1, x_2)\)</span> for r.v.’s <span class="math inline">\((X_1, X_2)\)</span> we can <em>marginalize</em> to either r.v., meaning we can compute the probabilities associated with just one of them. The ways we do this is by the law of total probability that we previously used to find probabilities of events using partitions. The marginal PMF of <span class="math inline">\(X_1\)</span> given <span class="math inline">\(p(x_1, x_2)\)</span> is given by
<span class="math display">\[p(x_1) = \sum_{x_2} p(x_1, \, x_2) = \sum_{x_2} P(X_1 = x_1, \, X_2 = x_2).\]</span>
What we’re doing here is partitioning the event <span class="math inline">\(\{X_1 = x_1\}\)</span> by values of <span class="math inline">\(X_2\)</span>, i.e.,
<span class="math display">\[\{X_1 = x_1\} = \bigcup_{x_2}\{\{X_1 = x_1\}\cap\{X_2 = x_2\}\}.\]</span>
<br>
Example: The marginal PMF of auto deductible is given by
<span class="math display">\[p(100) = p(100,0)+p(100,100)+p(100,200) = 0.20+0.10+0.20=0.50\]</span>
<span class="math display">\[p(250) = p(250,0)+p(250,100)+p(250,200) = 0.05+0.15+0.30=0.50\]</span>
<br></p>
<p>Example: In the die rolling example above we could find the PMF of the number of 1’s by using the general formula. On the other hand it’s easier if we recognize that the experiment in which we roll a die <span class="math inline">\(n\)</span> times and record the number of 1’s is equivalent to a coin-slipping experiment with a biased/unfair coin. Then, it must be that the PMF is
<span class="math display">\[p(x_1) = {n \choose x_1}(\tfrac16)^{x_1}(\tfrac56)^{n-x_1}.\]</span></p>
</div>
<div id="conditional-pmfs" class="section level3" number="7.1.2">
<h3><span class="header-section-number">7.1.2</span> Conditional PMFs</h3>
<p>The conditional PMF <span class="math inline">\(p(x_1|x_2)\)</span> assigns probabilities to the events <span class="math inline">\(\{X_1 = x_1\}\)</span> within the sample space <span class="math inline">\(\mathcal{S}\cap \{X_2 = x_2\}\)</span>. Since probabilities of discrete random variables behave just like probabilities of events, we have
<span class="math display">\[p(x_1|x_2) = \frac{p(x_1, \, x_2)}{p(x_2)},\]</span>
i.e., the conditional PMF of <span class="math inline">\(X_1\)</span> given <span class="math inline">\(X_2 = x_2\)</span> is the ratio of the joint PMF to the marginal PMF of <span class="math inline">\(X_2\)</span>. <br><br></p>
<p>Example: The conditional PMF of home deductible given auto deductible is given by the following table. And, notice the conditional PMF sums to 1 across rows because the rows represent the conditional sample spaces.</p>
<center>
<img src="cond_pmf.PNG" style="width:40.0%" />
</center>
<p><br>
Example: Using the formula, we can compute the conditional PMF of the number of 2’s in <span class="math inline">\(n\)</span> dice rolls given the number of 1’s (<span class="math inline">\(X_2\)</span> given <span class="math inline">\(X_1\)</span>).<br />
<span class="math display">\[\begin{align*}
p(x_2|x_1) &amp;= \frac{\frac{n!}{x_1! x_2! (n-x_1 -x_2)!}(\tfrac16)^{x_1}(\tfrac16)^{x_2}(\tfrac46)^{n-x_1-x_2}}{{n \choose x_1}(\tfrac16)^{x_1}(\tfrac56)^{n-x_1}}\\
&amp; = \frac{\frac{n!}{x_1! x_2! (n-x_1 -x_2)!}(\tfrac16)^{x_1}(\tfrac16)^{x_2}(\tfrac46)^{n-x_1-x_2}}{\frac{n!}{x_1!(n-x_1)!}(\tfrac16)^{x_1}(\tfrac56)^{n-x_1}}\\
&amp; = \frac{(n-x_1)!}{x_2!(n-x_1-x_2)!}(\tfrac16)^{x_2}(\tfrac46)^{n-x_1-x_2}(\tfrac65)^{n-x_1}\\
&amp;=\frac{(n-x_1)!}{x_2!(n-x_1-x_2)!}(\tfrac14)^{x_2}(\tfrac45)^{n-x_1}\\
&amp; = \frac{(n-x_1)!}{x_2!(n-x_1-x_2)!}(\tfrac14)^{x_2}(\tfrac45)^{n-x_1-x_2}(\tfrac{4}{5})^{x_2}\\
&amp; = \frac{(n-x_1)!}{x_2!(n-x_1-x_2)!}(\tfrac15)^{x_2}(\tfrac45)^{n-x_1-x_2}
\end{align*}\]</span>
In other words, the conditional distribution is the same as the probability of <span class="math inline">\(x_2\)</span> 2’s in <span class="math inline">\(n-x_1\)</span> rolls of a fair five-sided die with no 1-side!</p>
</div>
<div id="independence-of-discrete-random-variables" class="section level3" number="7.1.3">
<h3><span class="header-section-number">7.1.3</span> Independence of discrete random variables</h3>
<p>Two random variables <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent if their conditional PMF (either one) is equal to the corresponding marginal PMF. This implies the product rule holds, and that their Joint PMF is the product of their marginal PMFs. That is,</p>
<p><span class="math display">\[p(x_1|x_2) = p(x_1)\quad\text{or, equivalently}\quad p(x_1,x_2)=p(x_1)p(x_2), \quad \forall (x_1,x_2).\]</span>
<br></p>
<p>Example: Let <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> denote score ranges for two players on a bowling team. suppose that if the player scores less than 100 they denote it with a zero, between 100 and 200 a 1 and better than 200 a 3. Determine whether their performances are independent given their joint PMF:</p>
<center>
<p><img src="joint_pmf_ind.PNG" style="width:20.0%" /></p>
</center>
<p>By summing over the rows we obtain the marginal PMF of <span class="math inline">\(X_1\)</span>, given by <span class="math inline">\(p(x_1) = 0.1,\, 0.4,\, 0.5\)</span> for values <span class="math inline">\(0\)</span>, 1, and 2, respectively. Summing over columns we find the same marginal PMF for <span class="math inline">\(X_2\)</span>. Then, we have <span class="math inline">\(p(0,0) = 0.01 = 0.1\times 0.1 = p_{x_1}(0)p_{x_2}(0)\)</span>, <span class="math inline">\(p(1,0) = 0.04 = 0.4\times 0.1 = p_{x_1}(1)p_{x_2}(0)\)</span>, etc. If we check each of the 9 joint PMF values, we find that in <strong>every case</strong> the joint PMF equals the product of marginal PMF values. Conclude <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent.</p>
</div>
<div id="expectations-involving-multiple-discrete-random-variables" class="section level3" number="7.1.4">
<h3><span class="header-section-number">7.1.4</span> Expectations involving multiple discrete random variables</h3>
<p>Let <span class="math inline">\(g\)</span> be a function involving two random variables <span class="math inline">\(g:(X_1,X_2)\mapsto \mathbb{R}\)</span>. Then, the expectation of <span class="math inline">\(g(X_1,X_2)\)</span> is given by
<span class="math display">\[E(g(X_1,X_2)) = \sum_{x_1,x_2} g(x_1,x_2)p(x_1,x_2).\]</span></p>
<p><br><br></p>
<p>The covariance of <span class="math inline">\(X_1\)</span> amd <span class="math inline">\(X_2\)</span>, denoted <span class="math inline">\(Cov(X_1,X_2)\)</span> is defined <span class="math inline">\(E[(X_1 - E(X_1))(X_2 - E(X_2))]\)</span> where <span class="math inline">\(E(X_1)\)</span> and <span class="math inline">\(E(X_2)\)</span> are the marginal expectations, i.e., <span class="math inline">\(E(X_1) = \sum_{x_1} x_1 p(x_1)\)</span>. Covariance may be equivalently defined <span class="math inline">\(Cov(X_1, X_2) = E(X_1X_2)-E(X_1)E(X_2)\)</span>. If <span class="math inline">\((X_1,X_2)\)</span> are independent, then <span class="math inline">\(Cov(X_1,X_2) = 0\)</span>.</p>
<p><br><br></p>
<p>Example: Find the covariance of auto and home deductibles.<br>
The marginal distribution of home deductible is <span class="math inline">\(p(0) = 0.25\)</span>, <span class="math inline">\(p(100) = 0.25\)</span>, and <span class="math inline">\(p(200) = 0.50\)</span>. Then, the mean values of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are <span class="math inline">\(E(X_1) = 100*0.5 + 250 * 0.5 = 175\)</span> and <span class="math inline">\(E(X_2) = 125\)</span>. The expectation of their product is
<span class="math display">\[E(X_1X_2) = 0*100*0.2 + 100*100*0.1 + 100*200*0.2 + 0.*250*0.05 + 100*250*0.15 + 200*250*0.30=23750\]</span>
Their covariance is <span class="math inline">\(23750-175*125 = 1875\)</span>.</p>
<p>Like the variance, the covariance has units equal to the product of the units of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>, which may or may not be interpretable. A unitless alternative to covarianceis <em>correlation</em> which divides covariance by the product of standard deviations of the random variables:
<span class="math display">\[Corr(X_1, X_2) = \frac{Cov(X_1,X_2)}{\sigma_{X_1}\sigma_{X_2}}.\]</span>
Like covariance, correlation is zero for independent random variables. Unlike covariance, correlation is always between -1 and 1 and has no units. This gives it a meaningful scale for interpretation across applications.<br></p>
<p><br></p>
<p>Example: Find the correlation of auto and home deductibles.<br>
We need the variances of the auto and home deductibles, which we can compute by finding the second raw moments.
<span class="math display">\[E(X_1^2) = 100^2*0.5 + 250^2 * 0.5 = 36250\]</span>
So, <span class="math inline">\(\sigma_{X_1}^2 = 36250-175^2 = 5625\)</span>. And,
<span class="math display">\[E(X_2^2) = 100^2*0.25 + 200^2 * 0.5 = 22500\]</span>
so that <span class="math inline">\(\sigma_{X_2}^2 = 22500 - 125^2 = 6875\)</span>.
Their correlation is <span class="math inline">\(1875/(\sqrt{5625*6875}) \approx 0.3015\)</span>.</p>
</div>
</div>
<div id="jointly-distributed-continuous-random-variables" class="section level2" number="7.2">
<h2><span class="header-section-number">7.2</span> Jointly distributed continuous random variables</h2>
<p>Analogously to univariate continuous r.v.’s, the joint distribution of two (or more) continuous r.v.’s is characterized by its joint PDF <span class="math inline">\(f(x_1, x_2)\)</span>, and corresponding joint CDF <span class="math inline">\(F(x_1, x_2)\)</span> which determine joint probabilities via integration:
<span class="math display">\[P(X_1\leq x_1, X_2\leq x_2) =: F(x_1,x_2) = \int_{-\infty}^{x_1}\int_{-\infty}^{x_2}f(x_1,x_2)\,dx_2\,dx_1.\]</span>
<br>
Example: A business takes both online and in-person orders. Let <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> be the waiting times for the next two orders to be placed. Suppose
<span class="math display">\[f(x_1,x_2) = 2e^{-x_1-x_2}, \,\,0&lt;x_1&lt;x_2&lt;\infty.\]</span>
Find <span class="math inline">\(P(X_1\leq 1, X_2\leq 2)\)</span>.<br />
<span class="math display">\[\begin{align*}
P(X_1\leq 1, X_2\leq 2) &amp;= F(1,2) = \int_0^1\int_{x_1}^2 2e^{-x_1-x_2} \,dx_2\, dx_1\\
&amp; = 2\int_0^1 e^{-x_1}\left[\int_{x_1}^2 e^{-x_2} \,dx_2\right]\, dx_1\\
&amp; = 2\int_0^1 e^{-x_1} \left[ - e^{-x_2}|_{x_1}^2\right]dx_1\\
&amp; = 2\int_0^1 e^{-x_1}(e^{-x_1} - e^{-2})dx_1\\
&amp; = 2\int_0^1 e^{-2x_1} - e^{-x_1-2}dx_1\\
&amp; = 2[-\tfrac12 e^{-2x_1} + e^{-x_1-2}]|_{0}^1\\
&amp; = -e^{-2}+2e^{-3}+e^{-0}-2e^{-2}\\
&amp; \approx 0.693568
\end{align*}\]</span></p>
<div id="marginal-densities" class="section level3" number="7.2.1">
<h3><span class="header-section-number">7.2.1</span> Marginal densities</h3>
<p>Given a joint PDF of <span class="math inline">\((X_1, X_2)\)</span> the marginal PDFs determine the probabilities associated with each r.v. individually. That is, for <span class="math inline">\(X_1\)</span>
<span class="math display">\[f_{X_1}(x_1) = \int_{-\infty}^\infty f(x_1, x_2) dx_2\]</span>
is the marginal density of <span class="math inline">\(X_1\)</span> so that integration of the marginal PDF provides
<span class="math display">\[P(X_1\leq x_1) = F_{X_1}(x_1) = \int_{-\infty}^{x_1} f_{X_1}(t)dt.\]</span>
<br>
Example: For the above joint probability density of waiting times we get the marginal density of <span class="math inline">\(X_1\)</span>:
<span class="math display">\[\begin{align*}
f_{X_1}(x_1) &amp;=  2e^{-x_1}\int_{x_1}^{\infty}e^{-x_2}dx_2\\
&amp; = 2e^{-x_1}[-e^{-x_2}]|_{x_1}^\infty\\
&amp; = 2e^{-x_1}[0 + e^{-x_1}]\\
&amp; = 2e^{-2x_1}, \quad x_1&gt;0.
\end{align*}\]</span></p>
<p><br>
From the marginal PDF we can compute any kind of marginal expectation we want, like <span class="math inline">\(E(X_1)\)</span>, <span class="math inline">\(V(X_1)\)</span>, or <span class="math inline">\(M_{X_1}(t)\)</span>.</p>
<p><br>
Example: The moment generating function of the first arrival time is given by
<span class="math display">\[\begin{align*}
M_{X_1}(t) &amp;= \int_{0}^\infty 2e^{tx}e^{-2x}dx\\
&amp; = \int_{0}^\infty 2e^{-x(2-t)}dx \\
&amp; = 2\frac{e^{-x(2-t)}}{-(2-t)}|_{0}^\infty\\
&amp; = \frac{2}{2-t}, \quad t&lt;2.
\end{align*}\]</span>
And, the first (raw) moment then equals
<span class="math display">\[\begin{align*}
E(X_1) &amp;= \frac{d}{dt}[\frac{2}{2-t}]|_{t=0}\\
&amp;= \frac{2}{(2-t)^2}|_{t=0}\\
&amp; = 1/2.
\end{align*}\]</span></p>
</div>
<div id="conditional-densities" class="section level3" number="7.2.2">
<h3><span class="header-section-number">7.2.2</span> Conditional densities</h3>
<p>Given the joint and marginal densities <span class="math inline">\(f(x_1, x_2)\)</span> and <span class="math inline">\(f_{X_1}(x_1)\)</span> we can define the conditional density of <span class="math inline">\(X_2\)</span> given <span class="math inline">\(X_1=x_1\)</span> by
<span class="math display">\[f(x_2|x_1) = \frac{f(x_1, x_2)}{f_{X_1}(x_1)}\]</span>
which defines the probabilities
<span class="math display">\[P(X_2\leq x_2|X_1=x_1) = \int_{-\infty}^{x_2} f(t|x_1)dt.\]</span>
<br></p>
<p>Example: Given the first order arrives at time 1, what is the probability the second order arrives before time 2?
<span class="math display">\[f(x_2|x_1) = \frac{2e^{-x_1-x_2}}{2e^{-2x_1}} = \frac{e^{-x_2}}{e^{-x_1}}, \quad x_2&gt;x_1.\]</span></p>
<p><span class="math display">\[\begin{align*}
P(X_2\leq 2|X_1=1) &amp;= \int_{1}^2 \frac{e^{-x_2}}{e^{-1}}dx_2\\
&amp;= e^{1}[-e^{-x_2}]|_1^2\\
&amp; = e^{1}(e^{-1}-e^{-2})\\
&amp; = 1-e^{-1}\\
&amp; \approx 0.63212
\end{align*}\]</span></p>
<p>Example: Find the mean of the second arrival time, given the first arrival time is 1.</p>
<p><span class="math display">\[\begin{align*}
E(X_2|X_1=1) &amp;= \int_{1}^\infty x_2\frac{e^{-x_2}}{e^{-1}}dx_2\\
&amp; = e^{1}\int_{1}^\infty x_2e^{-x_2}dx_2\\
&amp; = e^1[-x_2e^{-x_2}|_1^\infty + \int_1^\infty e^{-x_2}dx_2] \quad \text{use by-parts}\\
&amp; = e^1[e^{-1} - e^{-x_2|_1^\infty}]\\
&amp; = 2
\end{align*}\]</span></p>
</div>
<div id="independence-of-jointly-distributed-continuous-r.v.s" class="section level3" number="7.2.3">
<h3><span class="header-section-number">7.2.3</span> Independence of jointly-distributed continuous r.v.’s</h3>
<p>Random variables <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent if <span class="math inline">\(f(x_1|x_2) = f_{X_1}(x_1)\)</span> for every <span class="math inline">\(x_2\)</span>; or, equivalently, if <span class="math inline">\(f(x_2|x_1) = f_{X_2}(x_2)\)</span> for every <span class="math inline">\(x_1\)</span>. <br><br></p>
<p>Example: A randomly selected student’s verbal score <span class="math inline">\(X_1\)</span> and quantitative score <span class="math inline">\(X_2\)</span> on a college entrance exam have the joint PDF
<span class="math display">\[f(x_1, x_2) = \tfrac25 (2x_1+3x_2), \quad 0\leq x_1\leq 1, \, 0\leq x_2\leq 1.\]</span>
Are they independent?</p>
<p><br><br>
Solution:
The marginal density of <span class="math inline">\(X_1\)</span> is given by
<span class="math display">\[\int_{0}^1 \tfrac25 (2x_1+3x_2)dx_2 = \tfrac25[2x_1x_2+1.5x_2^2]|_0^1 = \tfrac45 x_1 + 1.5\]</span></p>
<p>The marginal density of <span class="math inline">\(X_2\)</span> is given by
<span class="math display">\[\int_{0}^1 \tfrac25 (2x_1+3x_2)dx_1 = \tfrac25[\tfrac12x_1^2+3x_2x_1]|_0^1 = \tfrac15  + \tfrac65 x_2.\]</span>
The conditional density of <span class="math inline">\(X_2|X_1=x_1\)</span> is
<span class="math display">\[\frac{\tfrac25(2x_1+3x_2)}{\tfrac45 x_1+1.5} \ne f_{X_2}(x_2).\]</span>
They are not independent.</p>
<p>Example: For the waiting time distribution above, the marginal density of <span class="math inline">\(X_2\)</span> equals
<span class="math display">\[\begin{align*}
f_{X_2}(x_2) &amp;= \int_0^{x_2} 2e^{-x_1-x_2}dx_1\\
&amp; = 2e^{-x_2} - 2e^{-2x_2}, \quad x_2&gt;0.
\end{align*}\]</span></p>
<p>We previously found the conditional density <span class="math inline">\(f(x_2|x_1) = \frac{e^{-x_2}}{e^{-x_1}}, \,\,x_2&gt;x_1\)</span>. These two densities—the conditional and the <span class="math inline">\(X_2-\)</span>marginal— are not equal, so <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are not independent. Of course, it is clear they are dependent just by looking at their domain—since <span class="math inline">\(x_2\geq x_1\)</span> they must be dependent.</p>
</div>
<div id="expectations-involving-more-than-one-continuous-r.v." class="section level3" number="7.2.4">
<h3><span class="header-section-number">7.2.4</span> Expectations involving more than one continuous r.v.</h3>
<p>As in the discrete case, we can compute expectations of functions of two or more continuous r.v.’s:
<span class="math display">\[E(g(X_1,X_2)) = \int_{-\infty}^\infty\int_{-\infty}^\infty g(x,y)f(x,y) dxdy.\]</span></p>
<p>Example: Using the order times example we compute <span class="math inline">\(Cov(X_1, X_2)\)</span>, the covariance of the order times of the first and second orders:
<span class="math display">\[\begin{align*}
E(X_1X_2) &amp;= \int_0^\infty \int_{x_1}^\infty 2x_1x_2e^{-x_1-x_2} dx_2 dx_1\\
&amp; = 2\int_0^\infty x_1e^{-x_1} \left [\int_{x_1}^\infty x_2e^{-x_2}dx_2\right ] dx_1 \quad \text{use by-parts}\\
&amp; \text{set }u=x_2\, du = dx_2\, dv = e^{-x_2}\, v = -e^{-x_2}\, \\
&amp; = 2\int_0^\infty x_1e^{-x_1} \left [uv|_{x_1}^\infty - \int_{x_1}^\infty vdu \right] dx_1\\
&amp; = 2\int_0^\infty x_1e^{-x_1}(1+x_1)e^{-x_1}dx_1 \, \text{ have to do by-parts twice...}\\
&amp; = 1
\end{align*}\]</span></p>
<p>The mean of <span class="math inline">\(X_1\)</span> is
<span class="math display">\[\begin{align*}
E(X_1) &amp;= \int_0^{\infty} x_12e^{-2x_1} dx_1\\
&amp; = 1/2.
\end{align*}\]</span></p>
<p>Then,
<span class="math display">\[\begin{align*}
E(X_2) &amp;= \int_0^\infty 2x_2(e^{-x_2}-e^{-2x_2}) dx_2 \\
&amp; = 3/2 \quad \text{apply integratioon by parts}
\end{align*}\]</span></p>
<p>The covariance of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> is <span class="math inline">\(1 - (3/2)(1/2) =1/4\)</span>.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="expectation-of-random-variables.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="special-discrete-distributions.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/06-Joint-Distributions-of-Random-Variables.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
